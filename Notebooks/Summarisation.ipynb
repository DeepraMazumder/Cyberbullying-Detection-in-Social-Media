{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_E-JjAH0CRS2"
      },
      "outputs": [],
      "source": [
        "# Create and save the module\n",
        "module_code = \"\"\"\n",
        "from dotenv import load_dotenv\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Load the API key from .env file\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Configure the API key for Google Generative AI\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# Initialize the Generative Model\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
        "\n",
        "# Function to count words in the input text\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "# Define the prompt function\n",
        "def analyze_cyberbullying(text):\n",
        "    total_words = count_words(text)\n",
        "    prompt = (\n",
        "        \"Classify the given sentence into one of the following cyberbullying CATEGORIES: Race/Ethnicity related cyberbullying, \"\n",
        "        \"Gender/Sexual related cyberbullying, Religion related cyberbullying, or Other form of cyberbullying\\\\n\"\n",
        "        \"If the sentence is not cyberbullying, respond with 'Not Cyberbullying'\\\\n\"\n",
        "        \"SUGGESTED ALTERNATIVES: Suggest only 2 neutral/safer ways to express the sentence - no yapping\\\\n\"\n",
        "        \"HARMFUL CONTENT IDENTIFICATION: Display only the individual harmful words from the sentence as a list, \"\n",
        "        \"each marked with ðŸ”´. Do not include phrases, explanations, or additional text\\\\n\"\n",
        "        f\"TOTAL WORDS: {total_words}\\\\n\\\\n\"\n",
        "        \"FLAGGED PERCENTAGE: Display the percentage along with the breakdown like this: 20% (2 Harmful Words / 10 Total Words)\\\\n\"\n",
        "        \"REASON:\\\\nBriefly justify why the message was flagged and its cyberbullying category - no yapping\\\\n\"\n",
        "        f\"Sentence: {text}\\\\n\"\n",
        "    )\n",
        "\n",
        "    response = model.generate_content([prompt])\n",
        "    cleaned_output = re.sub(r'\\\\*\\\\*|\\\\*|## |\"', '', response.text)\n",
        "    return cleaned_output.strip()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohtPXBdvCbgB",
        "outputId": "635c1de6-7ec3-42ac-cd53-7e2999414a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Module saved successfully\n"
          ]
        }
      ],
      "source": [
        "# Save the module\n",
        "with open('../Artifacts/CyberbullyingSummarisation.py', 'w', encoding='utf-8') as file:\n",
        "    file.write(module_code)\n",
        "\n",
        "print(\"Module saved successfully\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
