{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import joblib\n",
    "import warnings\n",
    "import gc\n",
    "import wget\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download the helper script\n",
    "wget.download(\"https://raw.githubusercontent.com/yogawicaksana/helper_prabowo/main/helper_prabowo_ml.py\", out=\"../Artifacts/helper_prabowo_ml.py\")\n",
    "\n",
    "# Import the helper functions\n",
    "from helper_prabowo_ml import (\n",
    "    clean_html, punct, remove_digits, remove_links, \n",
    "    remove_special_characters, remove_, removeStopWords, \n",
    "    lower, email_address, non_ascii\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Dataset/OriginalDataset.csv')\n",
    "df = df.sample(n=20000).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['num_words'] = df['text'].apply(len)\n",
    "df['num_words'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='label', data=df, palette='Set2')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['tweet_length'] = df['text'].apply(len)\n",
    "\n",
    "sns.histplot(df['tweet_length'], kde=True, color='purple')\n",
    "plt.title('Tweet Distribution Analysis')\n",
    "plt.xlabel('Tweet Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x='label', y='tweet_length', data=df, palette='Set3')\n",
    "plt.title('Tweet Distribution Per Category')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Tweet Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_length = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_wordcloud_2x2(df):\n",
    "    categories = df['label'].unique()\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 6))\n",
    "\n",
    "    axs = axs.flatten()\n",
    "    for i, category in enumerate(categories):\n",
    "        category_data = df[df['label'] == category]['text']\n",
    "        wc = WordCloud(width=800, height=400, max_words=200, background_color='white').generate(' '.join(category_data))\n",
    "        axs[i].imshow(wc, interpolation='bilinear')\n",
    "        axs[i].set_title(f'WORD CLOUD FOR {category}')\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "generate_wordcloud_2x2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', max_features=20)\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "common_words = vectorizer.get_feature_names_out()\n",
    "word_counts = X.sum(axis=0).A1\n",
    "common_word_df = pd.DataFrame({'word': common_words, 'count': word_counts})\n",
    "\n",
    "sns.barplot(x='count', y='word', data=common_word_df.sort_values(by='count', ascending=False), palette='viridis')\n",
    "plt.title('Top 20 Most Common Words In Tweets')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_text(data,col):\n",
    "    data[col] = data[col].apply(func=remove_)\n",
    "    data[col] = data[col].apply(func=remove_digits)\n",
    "    data[col] = data[col].apply(func=remove_links)\n",
    "    data[col] = data[col].apply(func=remove_special_characters)\n",
    "    data[col] = data[col].apply(func=removeStopWords)\n",
    "    data[col] = data[col].apply(func=punct)\n",
    "    data[col] = data[col].apply(func=email_address)\n",
    "    data[col] = data[col].apply(func=non_ascii)\n",
    "    data[col] = data[col].apply(func=clean_html)\n",
    "    data[col] = data[col].apply(func=lower)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preprocessed_df = clean_text(df,'text')\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sreeniketh/cyberbullying_sentiment_dsce_2023')\n",
    "distilbert = TFAutoModelForSequenceClassification.from_pretrained('sreeniketh/cyberbullying_sentiment_dsce_2023',from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(preprocessed_df,test_size=0.3,random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_length = 85\n",
    "\n",
    "X_train = tokenizer(text=train_df['text'].tolist(),\n",
    "                   max_length=max_length,\n",
    "                   padding=True,\n",
    "                   truncation=True,\n",
    "                   add_special_tokens=True,\n",
    "                   return_tensors='tf',\n",
    "                   return_attention_mask=True,\n",
    "                   return_token_type_ids=False,\n",
    "                   verbose=1)\n",
    "\n",
    "X_test = tokenizer(text=test_df['text'].tolist(),\n",
    "                  max_length=max_length,\n",
    "                  padding=True,\n",
    "                  truncation=True,\n",
    "                  add_special_tokens=True,\n",
    "                  return_tensors='tf',\n",
    "                  return_attention_mask=True,\n",
    "                  return_token_type_ids=False,\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode labels and store mapping  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "train_df.label = encoder.fit_transform(train_df.label)\n",
    "test_df.label = encoder.transform(test_df.label)\n",
    "\n",
    "encoded_labels = dict()\n",
    "\n",
    "for idx, label in enumerate(encoder.classes_):\n",
    "    encoded_labels[idx] = label\n",
    "\n",
    "encoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_ids = Input(shape=(max_length,),name='input_ids',dtype=tf.int32)\n",
    "attention_mask = Input(shape=(max_length,),name='attention_mask',dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract embeddings from DistilBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embeddings = distilbert(input_ids,attention_mask=attention_mask)[0] # 0 -> final hidden state output, 1 -> pooling output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output = Flatten()(embeddings)\n",
    "output = Dense(units=128,activation='relu')(output)\n",
    "output = BatchNormalization()(output)\n",
    "output = Dropout(0.3)(output)\n",
    "output = Dense(units=64,activation='relu')(output)\n",
    "output = BatchNormalization()(output)\n",
    "output = Dropout(0.2)(output)\n",
    "output = Dense(units=32,activation='relu')(output)\n",
    "output = BatchNormalization()(output)\n",
    "output = Dropout(0.1)(output)\n",
    "output = Dense(units=4,activation='softmax')(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_ids,attention_mask],outputs=output)\n",
    "model.layers[2].trainable = True\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_model(model,to_file='model.png',show_shapes=True,dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer and class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "adam = AdamW(learning_rate=5e-5, weight_decay=0.01, epsilon=2e-7, clipnorm=1.0)\n",
    "class_weights = {0: 0.3, 1: 0.25, 2: 0.15, 3: 0.3}\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=adam,\n",
    "    metrics=[SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate decay function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def time_based_decay(epoch, lr):\n",
    "    decay_rate = 1e-5\n",
    "    decay_epoch = 10\n",
    "    return lr * (1 / (1 + decay_rate * epoch / decay_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_sparse_categorical_accuracy',mode='max',patience=5,restore_best_weights=True,verbose=1)\n",
    "ls = LearningRateScheduler(time_based_decay,verbose=1)\n",
    "mc = ModelCheckpoint(filepath='cyberbullying_classifier.keras',monitor='val_sparse_categorical_accuracy',save_best_only=True,mode='max',verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "r = model.fit(x={'input_ids': X_train['input_ids'], 'attention_mask': X_train['attention_mask']},\n",
    "              y=train_df.label,\n",
    "              batch_size=256,\n",
    "              epochs=15,\n",
    "              class_weight=class_weights,\n",
    "              validation_data=({'input_ids': X_test['input_ids'], 'attention_mask': X_test['attention_mask']},test_df.label),\n",
    "              callbacks=[es,ls,mc]\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot training loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(r.history['loss'],'r',label='Train Loss')\n",
    "plt.plot(r.history['val_loss'],'b',label='Test Loss')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Graph')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(r.history['sparse_categorical_accuracy'],'r',label='Train Accuracy')\n",
    "plt.plot(r.history['val_sparse_categorical_accuracy'],'b',label='Test Accuracy')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Graph')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate({'input_ids': X_test['input_ids'],'attention_mask': X_test['attention_mask']},test_df.label)\n",
    "print(\"Sparse Categorical Crossentropy Loss:\", round(loss,2))\n",
    "print(\"Sparse Categorical Accuracy:\", round(acc*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_predictions = model.predict({'input_ids': X_test['input_ids'],'attention_mask': X_test['attention_mask']})\n",
    "test_predictions = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "cm = confusion_matrix(test_df.label, test_predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "heatmap = sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', xticklabels=True, yticklabels=True)\n",
    "colorbar = heatmap.collections[0].colorbar\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_df.label,test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save essential files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preprocessed_df.to_csv('../Dataset/PreprocessedDataset.csv', index=False)\n",
    "print(\"Preprocessed dataframe saved as CSV\")\n",
    "\n",
    "with open('../Artifacts/X_train.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "print(\"X_train saved as pickle\")\n",
    "\n",
    "with open('../Artifacts/X_test.pkl', 'wb') as f:\n",
    "    pickle.dump(X_test, f)\n",
    "print(\"X_test saved as pickle\")\n",
    "\n",
    "with open('../Artifacts/LabelEncoder.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder, f)\n",
    "print(\"Label encoder saved using pickle\")\n",
    "\n",
    "model.save('../Artifacts/CyberbullyingClassifier.keras')\n",
    "print(\"Model saved as .keras\")\n",
    "\n",
    "with open('../Artifacts/TrainingHistory.pkl', 'wb') as f:\n",
    "    pickle.dump(r.history, f)\n",
    "print(\"Training history saved as pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load essential files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preprocessed_df = pd.read_csv('../Dataset/PreprocessedDataset.csv')\n",
    "print(\"Preprocessed dataframe loaded from CSV\")\n",
    "\n",
    "with open('../Artifacts/X_train.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "print(\"X_train loaded from pickle\")\n",
    "\n",
    "with open('../Artifacts/X_test.pkl', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "print(\"X_test loaded from pickle\")\n",
    "\n",
    "with open('../Artifacts/LabelEncoder.pkl', 'rb') as f:\n",
    "    encoder = pickle.load(f)\n",
    "print(\"Label encoder loaded using pickle\")\n",
    "\n",
    "with open('../Artifacts/TrainingHistory.pkl', 'rb') as f:\n",
    "    training_history = pickle.load(f)\n",
    "print(\"Training history loaded from pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "custom_objects = {\n",
    "    'TFDistilBertForSequenceClassification': TFDistilBertForSequenceClassification\n",
    "}\n",
    "\n",
    "model = load_model('../Artifacts/CyberbullyingClassifier.keras', custom_objects=custom_objects)\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3560090,
     "sourceId": 6200876,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
