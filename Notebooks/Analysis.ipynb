{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n9SRvq_vWr2"
      },
      "outputs": [],
      "source": [
        "# Add module path\n",
        "import sys\n",
        "sys.path.append('../Artifacts')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCqUreqVvWuX"
      },
      "outputs": [],
      "source": [
        "# Import the saved function\n",
        "from CyberbullyingSummarisation import analyze_cyberbullying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "VSjldIfQCopP",
        "outputId": "15c98b93-0c80-4f4e-fa2a-15805e072482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence to classify: Her husband is a nigga and her child looks like a monkey\n"
          ]
        }
      ],
      "source": [
        "# Get user input and analyze\n",
        "user_input = input(\"Enter a sentence to classify: \")\n",
        "result = analyze_cyberbullying(user_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RIh-J_HCor3",
        "outputId": "baaad95f-0052-4ac0-b246-27e41b175c1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INPUT SENTENCE: Her husband is a nigga and her child looks like a monkey\n",
            "\n",
            "CATEGORY: Race/Ethnicity related cyberbullying\n",
            "\n",
            "SUGGESTED ALTERNATIVES:\n",
            "\n",
            "1.  Her husband's heritage is different.\n",
            "2.  Her child has unique features.\n",
            "\n",
            "\n",
            "HARMFUL CONTENT IDENTIFICATION:\n",
            "\n",
            " ðŸ”´ nigga\n",
            " ðŸ”´ monkey\n",
            "\n",
            "\n",
            "TOTAL WORDS: 12\n",
            "\n",
            "FLAGGED PERCENTAGE: 16.67% (2 Harmful Words / 12 Total Words)\n",
            "\n",
            "REASON:\n",
            "\n",
            "The sentence uses racial slurs (nigga and monkey) to insult and dehumanize the individuals mentioned.  This constitutes race/ethnicity related cyberbullying.\n"
          ]
        }
      ],
      "source": [
        "# Combine user input and result for display and saving\n",
        "final_output = f\"INPUT SENTENCE: {user_input}\\n\\n{result}\"\n",
        "\n",
        "# Print the result\n",
        "print(final_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CReqRF4JDBoo",
        "outputId": "d2878dbc-0038-41e3-f661-e0a9e5718871"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary saved successfully\n"
          ]
        }
      ],
      "source": [
        "# Save the summary to a file\n",
        "with open('../Artifacts/CyberbullyingSummary.txt', 'w') as f:\n",
        "    f.write(final_output)\n",
        "\n",
        "print(f\"Summary saved successfully\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
